
  

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deployment &mdash; MLflow 2.9.3.dev0 documentation</title>
  
   
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="canonical" href="https://mlflow.org/docs/latest/deployment/index.html">
  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    

    

  
    

  

  
  
    

  

  
  
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600" rel="stylesheet">
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/cards.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/grids.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/mobile.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/simple-cards.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/tabs.css" type="text/css" />
    
  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="MLflow 2.9.3.dev0 documentation" href="../index.html"/>
        <link rel="next" title="Deploy MLflow Model as a Local Inference Server" href="/deploy-model-locally.html"/>
        <link rel="prev" title="Customizing a Model’s predict method" href="/../traditional-ml/creating-custom-pyfunc/notebooks/override-predict.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>
<script type="text/javascript" src="../_static/jquery.js"></script>
<script type="text/javascript" src="../_static/underscore.js"></script>
<script type="text/javascript" src="../_static/doctools.js"></script>
<script type="text/javascript" src="../_static/tabs.js"></script>
<script type="text/javascript" src="../_static/languagesections.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>

<body class="wy-body-for-nav" role="document">
  

  
  <nav class="wy-nav-top header" role="navigation" aria-label="top navigation">
    <ul>
  <li class="menu-toggle">
    <i data-toggle="wy-nav-top" class="wy-nav-top-menu-button db-icon db-icon-menu pull-left"></i>
    <a href="../index.html" class="wy-nav-top-logo"
      ><img src="../_static/MLflow-logo-final-black.png" alt="MLflow"
    /></a>
    <span class="version">2.9.3.dev0</span>
  </li>
</ul>
  </nav>
  <page>
    

    <nav data-toggle="wy-nav-shift" class="wy-nav-side relative">
      <div class="wy-side-scroll">
  <div class="wy-side-nav-search">
    

    

    
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
  </form>
</div>


    
  </div>

  <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
    
      <a href="../index.html" class="main-navigation-home"><img src="../_static/icons/nav-home.svg"> MLflow</a>
    

    
      

      
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">What is MLflow?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/index.html">Getting Started with MLflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../new-features/index.html">New Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llms/index.html">LLMs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model-evaluation/index.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep-learning/index.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../traditional-ml/index.html">Traditional ML</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#concepts">Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">MLflow Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#container">Container</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deployment-target">Deployment Target</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-it-works">How it works</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-deployment-targets">Supported Deployment Targets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="deploy-model-locally.html">Deploy MLflow Model as a Local Inference Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="deploy-model-to-sagemaker.html">Deploy MLflow Model to Amazon SageMaker</a></li>
<li class="toctree-l3"><a class="reference internal" href="deploy-model-to-kubernetes/index.html">Deploy MLflow Model to Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-references">API References</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l3"><a class="reference internal" href="#python-apis">Python APIs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#faq">FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-to-test-my-model-before-deploying-to-the-production-environment">How to test my model before deploying to the production environment?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#testing-offline-prediction-with-a-virtual-environment">Testing offline prediction with a virtual environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-online-inference-endpoint-with-a-virtual-environment">Testing online inference endpoint with a virtual environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#testing-online-inference-endpoint-with-a-docker-container">Testing online inference endpoint with a Docker container</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-fix-dependency-errors-when-serving-my-model">How to fix dependency errors when serving my model?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#check-the-missing-dependencies">1. Check the missing dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#try-adding-the-dependencies-using-the-mlflow-models-predict-api">2. Try adding the dependencies using the <cite>mlflow models predict</cite> API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#update-the-model-metadata">3. Update the model metadata</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tracking.html">MLflow Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system-metrics/index.html">System Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../projects.html">MLflow Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models.html">MLflow Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model-registry.html">MLflow Model Registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes.html">MLflow Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins.html">MLflow Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auth/index.html">MLflow Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli.html">Command-Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search-runs.html">Search Runs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../search-experiments.html">Search Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../R-api.html">R API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../java_api/index.html">Java API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rest-api.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docker.html">Official MLflow Docker Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community-model-flavors.html">Community Model Flavors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials-and-examples/index.html">Tutorials and Examples</a></li>
</ul>

      
    
  </div>

  <div role="contentinfo">
    

    <p>
      <a id='feedbacklink' href="https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.md" target="_blank">Contribute</a>
    </p>
  </div>
</div>
    </nav>

    <main class="wy-grid-for-nav">
      <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
        <div class="wy-nav-content">
          <div class="rst-content">
            










<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Documentation</a> <span class="db-icon db-icon-chevron-right"></span></li>
    
    
      <li>Deployment</li>
    
    
    <!-- <li class="wy-breadcrumbs-aside">
      <a href="https://github.com/mlflow/mlflow/blob/master/docs/source/deployment/index.rst" class="fa fa-github"> Edit on GitHub</a>
    </li> -->
    
  </ul>
</div>
            <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
              <div itemprop="articleBody">
                
  <div class="section" id="deployment">
<h1>Deployment<a class="headerlink" href="#deployment" title="Permalink to this headline"> </a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This page describes the toolset for deploying your in-house MLflow Model. For information on the <strong>LLM Deployment Server</strong> (formerly known as AI Gateway), please refer to <a class="reference external" href="../llms/deployments/index.html">MLflow Deployment Server</a>.</p>
</div>
<p>After training your machine learning model and ensuring its performance, the next step is deploying it to a production environment.
This process can be complex, but MLflow simplifies it by offering an easy toolset for deploying your ML models to various targets, including local environments, cloud services, and Kubernetes clusters.</p>
<div class="figure align-center" style="width: 90%">
<img alt="../_images/mlflow-deployment-overview.png" src="../_images/mlflow-deployment-overview.png" />
</div>
<p>By using MLflow deployment toolset, you can enjoy the following benefits:</p>
<ul class="simple">
<li><p><strong>Effortless Deployment</strong>: MLflow provides a simple interface for deploying models to various targets, eliminating the need to write boilorplate code.</p></li>
<li><p><strong>Dependency and Environment Management</strong>: MLflow ensures that the deployment environment mirrors the training environment, capturing all dependencies. This guarantees that models run consistently, regardless of where they’re deployed.</p></li>
<li><p><strong>Packaging Models and Code</strong>: With MLflow, not just the model, but any supplementary code and configurations are packaged along with the deployment container. This ensures that the model can be executed seamlessly without any missing components.</p></li>
<li><p><strong>Avoid Vendor Lock-in</strong>: MLflow provides a standard format for packaging models and unified APIs for deployment. You can easily switch between deployment targets without having to rewrite your code.</p></li>
</ul>
<div class="section" id="concepts">
<h2>Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline"> </a></h2>
<div class="section" id="id1">
<h3><a class="reference external" href="../models.html">MLflow Model</a><a class="headerlink" href="#id1" title="Permalink to this headline"> </a></h3>
<p><a class="reference external" href="../models.html">MLflow Model</a> is a standard format that packages a machine learning model with its metadata, such as dependencies and inference schema.
You typically create a model as a result of training execution using the <a class="reference external" href="../tracking.html">MLflow Tracking APIs</a>, for instance, <a class="reference internal" href="../python_api/mlflow.pyfunc.html#mlflow.pyfunc.log_model" title="mlflow.pyfunc.log_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.pyfunc.log_model()</span></code></a>.
Alternatively, models can be registered and retrieved via the <a class="reference external" href="../model-registry.html">MLflow Model Registry</a>.
To use MLflow deployment, you must first create a model.</p>
</div>
<div class="section" id="container">
<h3>Container<a class="headerlink" href="#container" title="Permalink to this headline"> </a></h3>
<p>Container plays a critical role for simplifying and standardizing the model deployment process. MLflow uses Docker containers to package models with their dependencies,
enabling deployment to various destinations without environment compatibility issues.
If you’re new to Docker, you can learn more at <a class="reference external" href="https://www.docker.com/resources/what-container//">“What is a Container”</a>.</p>
</div>
<div class="section" id="deployment-target">
<h3>Deployment Target<a class="headerlink" href="#deployment-target" title="Permalink to this headline"> </a></h3>
<p>Deployment target refers to the destination environment for your model. MLflow supports various targets, including local environments, cloud services (AWS, Azure), Kubernetes clusters, and others.</p>
</div>
</div>
<div class="section" id="how-it-works">
<h2>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline"> </a></h2>
<p>An <a class="reference external" href="../models.html">MLflow Model</a> already packages your model and its dependencies, hence MLflow can create either a virtual environment (for local deployment)
or a Docker container image containing everything needed to run your model. Subsequently, MLflow launches an inference server with REST endpoints using
frameworks like <a class="reference external" href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>, preparing it for deployment to various destinations to handle inference requests.
Detailed information about the server and endpoints is available in <a class="reference internal" href="deploy-model-locally.html#local-inference-server-spec"><span class="std std-ref">Inference Server Specification</span></a>.</p>
<p>MLflow provides <a class="reference internal" href="#deployment-cli"><span class="std std-ref">CLI commands</span></a> and <a class="reference internal" href="#deployment-python-api"><span class="std std-ref">Python APIs</span></a> to facilitate the deployment process.
The required commands differ based on the deployment target, so please continue reading to the next section for more details about your specific target.</p>
</div>
<div class="section" id="supported-deployment-targets">
<h2>Supported Deployment Targets<a class="headerlink" href="#supported-deployment-targets" title="Permalink to this headline"> </a></h2>
<p>MLflow offers support for a variety of deployment targets. For detailed information and tutorials on each, please follow the respective links below.</p>
<div class="toctree-wrapper compound">
</div>
<section>
    <article class="simple-grid">
        <div class="simple-card">
            <a href="deploy-model-locally.html">
                <div class="header-with-image">
                    Deploying a Model Locally
                </div>
                <p>
                   Deploying a model locally as an inference server is straightforward with MLflow, requiring just a single command <code>mlflow models serve</code>.
                </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="deploy-model-to-sagemaker.html">
                <div class="header-with-image">
                    <img src="../_static/images/logos/amazon-sagemaker-logo.png" alt="Amazon SageMaker Logo" />
                </div>
                <p>
                    Amazon SageMaker is a fully managed service for scaling ML inference containers.
                    MLflow simplifies the deployment process with easy-to-use commands, eliminating the need to write container definitions.
                </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-mlflow-models">
                <div class="header-with-image">
                    <img src="../_static/images/logos/azure-ml-logo.png" alt="AzureML Logo" style="width: 90%"/>
                </div>
                <p>
                    MLflow integrates seamlessly with Azure ML. You can deploy MLflow Model to the Azure ML managed online/batch endpoints,
                    or to Azure Container Instances (ACI) / Azure Kubernetes Service (AKS).
                </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="https://docs.databricks.com/en/mlflow/models.html">
                <div class="header-with-image">
                    <img src="../_static/images/logos/databricks-logo.png" alt="Databricks Logo" style="width: 90%"/>
                </div>
                <p>
                    Databricks Model Serving offers a fully managed service for serving MLflow models at scale,
                    with added benefits of performance optimizations and monitoring capabilities.
                </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="deploy-model-to-kubernetes/index.html">
                <div class="header-with-image">
                    <img src="../_static/images/logos/kubernetes-logo.png" alt="Kubernetes Logo" style="width: 90%"/>
                </div>
                <p>
                   MLflow Deployment integrates with Kubernetes-native ML serving frameworks
                   such as Seldon Core and KServe (formerly KFServing).
                 </p>
            </a>
        </div>
        <div class="simple-card">
            <a href="../plugins.html#deployment-plugins">
                <div class="header-with-image">
                    Community Supported Targets
                </div>
                <p>
                    MLflow also supports more deployment targets such as Ray Serve, Redis AI, Torch Serve, Oracle Cloud Infrastructure (OCI), through community-supported plugins.
                </p>
            </a>
        </div>
    </article>
</section></div>
<div class="section" id="api-references">
<h2>API References<a class="headerlink" href="#api-references" title="Permalink to this headline"> </a></h2>
<div class="section" id="command-line-interface">
<span id="deployment-cli"></span><h3>Command Line Interface<a class="headerlink" href="#command-line-interface" title="Permalink to this headline"> </a></h3>
<p>Deployment-related commands are primarily categorized under two modules:</p>
<ul class="simple">
<li><p><a class="reference external" href="../cli.html#mlflow-models">mlflow models</a> - typically used for local deployment.</p></li>
<li><p><a class="reference external" href="../cli.html#mlflow-deployments">mlflow deployments</a> - typically used for deploying to custom targets.</p></li>
</ul>
<p>Note that these categories are not strictly separated and may overlap. Furthermore, certain targets require
custom modules or plugins, for example, <a class="reference external" href="../cli.html#mlflow-sagemaker">mlflow sagemaker</a> is used for Amazon
SageMaker deployments, and the <a class="reference external" href="https://pypi.org/project/azureml-mlflow/">azureml-mlflow</a> library is required for Azure ML.</p>
<p>Therefore, it is advisable to consult the specific documentation for your chosen target to identify the appropriate commands.</p>
</div>
<div class="section" id="python-apis">
<span id="deployment-python-api"></span><h3>Python APIs<a class="headerlink" href="#python-apis" title="Permalink to this headline"> </a></h3>
<p>Almost all functionalities available in MLflow deployment can also be accessed via Python APIs. For more details, refer to the following API references:</p>
<ul class="simple">
<li><p><a class="reference external" href="../python_api/mlflow.models.html">mlflow.models</a></p></li>
<li><p><a class="reference external" href="../python_api/mlflow.deployments.html">mlflow.deployments</a></p></li>
<li><p><a class="reference external" href="../python_api/mlflow.sagemaker.html">mlflow.sagemaker</a></p></li>
</ul>
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"> </a></h2>
<div class="section" id="how-to-test-my-model-before-deploying-to-the-production-environment">
<h3>How to test my model before deploying to the production environment?<a class="headerlink" href="#how-to-test-my-model-before-deploying-to-the-production-environment" title="Permalink to this headline"> </a></h3>
<p>Testing your model before deployment is a critical step to ensure production readiness.
MLflow provides a few ways to test your model locally, either in a virtual environment or a Docker container.</p>
<div class="section" id="testing-offline-prediction-with-a-virtual-environment">
<h4>Testing offline prediction with a virtual environment<a class="headerlink" href="#testing-offline-prediction-with-a-virtual-environment" title="Permalink to this headline"> </a></h4>
<p>You can use <cite>mlflow models predict</cite> API via CLI or Python to making test predictions with your model.
This wiil load your model from the model URI, create a virtual environment with the model dependencies (defined in MLflow Model),
and run offline predictions with the model.
Please refer to <a class="reference internal" href="../python_api/mlflow.models.html#mlflow.models.predict" title="mlflow.models.predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">mlflow.models.predict()</span></code></a> or <a class="reference external" href="../cli.html#mlflow-models">CLI reference</a> for more detailed usage for the <cite>predict</cite> API.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-0-QmFzaA==" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-0-QmFzaA==" name="QmFzaA==" role="tab" tabindex="0">Bash</button><button aria-controls="panel-0-UHl0aG9u" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-0-UHl0aG9u" name="UHl0aG9u" role="tab" tabindex="-1">Python</button></div><div aria-labelledby="tab-0-QmFzaA==" class="sphinx-tabs-panel code-tab group-tab" id="panel-0-QmFzaA==" name="QmFzaA==" role="tabpanel" tabindex="0"><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>predict<span class="w"> </span>-m<span class="w"> </span>runs:/&lt;run_id&gt;/model-i<span class="w"> </span>&lt;input_path&gt;
</pre></div>
</div>
</div><div aria-labelledby="tab-0-UHl0aG9u" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-0-UHl0aG9u" name="UHl0aG9u" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="s2">&quot;runs:/&lt;run_id&gt;/model&quot;</span><span class="p">,</span>
    <span class="n">input_data</span><span class="o">=&lt;</span><span class="n">input_data</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div></div>
<p>Using the <cite>predict</cite> API is convenient for testing your model and inference environment quickly.
However, it is not a perfect simulation of the serving because it does not start the online inference server.</p>
</div>
<div class="section" id="testing-online-inference-endpoint-with-a-virtual-environment">
<h4>Testing online inference endpoint with a virtual environment<a class="headerlink" href="#testing-online-inference-endpoint-with-a-virtual-environment" title="Permalink to this headline"> </a></h4>
<p>If you want to test your model with actually running the online inference server, you can use MLflow <cite>serve</cite> API.
This will create a virtual environment with your model and dependencies, similarly to the <cite>predict</cite> API, but will start the inference server
and expose the REST endpoints. Then you can send a test request and validate the response.
Please refer to <a class="reference external" href="../cli.html#mlflow-models">CLI reference</a> for more detailed usage for the <cite>serve</cite> API.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>serve<span class="w"> </span>-m<span class="w"> </span>runs:/&lt;run_id&gt;/model<span class="w"> </span>-p<span class="w"> </span>&lt;port&gt;

<span class="c1"># In another terminal</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--data<span class="w"> </span><span class="s1">&#39;{&quot;inputs&quot;: [[1, 2], [3, 4]]}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>http://localhost:&lt;port&gt;/invocations
</pre></div>
</div>
<p>While this is reliable way to test your model before deployment, one caveat is that virtual environment doesn’t absorb the OS level differences
between your machine and the production environment. For example, if you are using MacOS as a local dev machine but your deployment target is
running on Linux, you may encounter some issues that are not reproducible in the virtual environment.</p>
<p>In this case, you can use Docker container to test your model. While it doesn’t provide full OS level isolation unlike virtual machine e.g. we
can’t run Windows container on Linux machine, Docker covers some popular test scenario such as running different versions of Linux or simulating
Linux environment on Mac/Windows.</p>
</div>
<div class="section" id="testing-online-inference-endpoint-with-a-docker-container">
<h4>Testing online inference endpoint with a Docker container<a class="headerlink" href="#testing-online-inference-endpoint-with-a-docker-container" title="Permalink to this headline"> </a></h4>
<p>MLflow <cite>build-docker</cite> API for CLI and Python, which builds an Ubuntu-based Docker image for serving your mdoel.
The image will contain your model and dependencies and has an entrypoint to start the inference server. Similarly to the <cite>serve</cite> API,
you can send a test request and validate the response.
Please refer to <a class="reference external" href="../cli.html#mlflow-models">CLI reference</a> for more detailed usage for the <cite>build-docker</cite> API.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>build-docker<span class="w"> </span>-m<span class="w"> </span>runs:/&lt;run_id&gt;/model<span class="w"> </span>-n<span class="w"> </span>&lt;image_name&gt;

docker<span class="w"> </span>run<span class="w"> </span>-p<span class="w"> </span>&lt;port&gt;:8080<span class="w"> </span>&lt;image_name&gt;

<span class="c1"># In another terminal</span>
curl<span class="w"> </span>-X<span class="w"> </span>POST<span class="w"> </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--data<span class="w"> </span><span class="s1">&#39;{&quot;inputs&quot;: [[1, 2], [3, 4]]}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>http://localhost:&lt;port&gt;/invocations
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-to-fix-dependency-errors-when-serving-my-model">
<h3>How to fix dependency errors when serving my model?<a class="headerlink" href="#how-to-fix-dependency-errors-when-serving-my-model" title="Permalink to this headline"> </a></h3>
<p>One common issue during model deployment is dependency issue. When logging/saving your model, MLflow tries to infer the
model dependencies and save them as part of the MLflow Model metadata. However, this inference is not always accurate and may
miss some dependencies. This can cause errors when serving your model, such as “ModuleNotFoundError” or “ImportError”. Here
is the steps to fix missing dependencies error.</p>
<div class="section" id="check-the-missing-dependencies">
<h4>1. Check the missing dependencies<a class="headerlink" href="#check-the-missing-dependencies" title="Permalink to this headline"> </a></h4>
<p>The missing dependencies are listed in the error message. For example, if you see the following error message:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ModuleNotFoundError:<span class="w"> </span>No<span class="w"> </span>module<span class="w"> </span>named<span class="w"> </span><span class="s1">&#39;cv2&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="try-adding-the-dependencies-using-the-mlflow-models-predict-api">
<h4>2. Try adding the dependencies using the <cite>mlflow models predict</cite> API<a class="headerlink" href="#try-adding-the-dependencies-using-the-mlflow-models-predict-api" title="Permalink to this headline"> </a></h4>
<p>Now that you know the missing dependencies, you can create a new model version with the correct dependencies.
However, creating a new model for trying new dependencies might be a bit tedious, particularly because you may need to
iterate multiple times to find the correct solution. Instead, you can use the <cite>predict</cite> API to test your change without
actually mutating the model.</p>
<p>To do so, use the <cite>pip-requirements-override</cite> option to specify pip dependencies like <cite>opencv-python==4.8.0</cite>.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" role="tablist"><button aria-controls="panel-1-QmFzaA==" aria-selected="true" class="sphinx-tabs-tab code-tab group-tab" id="tab-1-QmFzaA==" name="QmFzaA==" role="tab" tabindex="0">Bash</button><button aria-controls="panel-1-UHl0aG9u" aria-selected="false" class="sphinx-tabs-tab code-tab group-tab" id="tab-1-UHl0aG9u" name="UHl0aG9u" role="tab" tabindex="-1">Python</button></div><div aria-labelledby="tab-1-QmFzaA==" class="sphinx-tabs-panel code-tab group-tab" id="panel-1-QmFzaA==" name="QmFzaA==" role="tabpanel" tabindex="0"><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mlflow<span class="w"> </span>models<span class="w"> </span>predict<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-m<span class="w"> </span>runs:/&lt;run_id&gt;/model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-i<span class="w"> </span>&lt;input_path&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pip-requirements-override<span class="w"> </span>opencv-python<span class="o">==</span><span class="m">4</span>.8.0
</pre></div>
</div>
</div><div aria-labelledby="tab-1-UHl0aG9u" class="sphinx-tabs-panel code-tab group-tab" hidden="true" id="panel-1-UHl0aG9u" name="UHl0aG9u" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">model_uri</span><span class="o">=</span><span class="s2">&quot;runs:/&lt;run_id&gt;/model&quot;</span><span class="p">,</span>
    <span class="n">input_data</span><span class="o">=&lt;</span><span class="n">input_data</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">pip_requirements</span><span class="o">=</span><span class="s2">&quot;opencv-python==4.8.0&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div></div>
<p>The specified dependencies will be installed to the virtual environment in addition to (or instead of) the dependencies
defined in the model metadata. Since this doesn’t mutate the model, you can iterate quickly and safely to find the correct dependencies.</p>
</div>
<div class="section" id="update-the-model-metadata">
<h4>3. Update the model metadata<a class="headerlink" href="#update-the-model-metadata" title="Permalink to this headline"> </a></h4>
<p>Once you find the correct dependencies, you can create a new model with the correct dependencies.
To do so, specify <cite>extra_pip_requirements</cite> option when logging the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mlflow</span>

<span class="n">mlflow</span><span class="o">.</span><span class="n">pyfunc</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span>
    <span class="n">artifact_path</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span>
    <span class="n">python_model</span><span class="o">=</span><span class="n">python_model</span><span class="p">,</span>
    <span class="c1"># If you want to define all dependencies from scratch, use `pip_requirements` option.</span>
    <span class="c1"># Both options also accept a path to a pip requirements file e.g. requirements.txt.</span>
    <span class="n">extra_pip_requirements</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;opencv-python==4.8.0&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Alternatively, you can manually edit the model metadata stored in the artifact storage. For example, <cite>&lt;path-to-model&gt;/requirements.txt</cite>
defines pip dependencies to be installed for serving your model (or <cite>&lt;path-to-model&gt;/conda.yaml</cite> if you use conda as a package manager).
However, this approach is not recommended because this is error-prone and also you need to do this for every model version.</p>
</div>
</div>
</div>
</div>
</div>


              </div>
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../traditional-ml/creating-custom-pyfunc/notebooks/override-predict.html" class="btn btn-neutral" title="Customizing a Model’s predict method" accesskey="p"><span class="db-icon db-icon-chevron-left"></span> Previous</a>
      
      
        <a href="deploy-model-locally.html" class="btn btn-neutral" title="Deploy MLflow Model as a Local Inference Server" accesskey="n">Next <span class="db-icon db-icon-chevron-right"></span></a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
      <p class="copyright">
          &copy; MLflow Project, a Series of LF Projects, LLC. All rights reserved.
      </p>

  </div> 

</footer>
          </div>
        </div>
      </section>
    </main>
  </page>

  


  
  <script type="text/javascript">
    var DOCUMENTATION_OPTIONS = {
      URL_ROOT:'../',
      VERSION:'2.9.3.dev0',
      COLLAPSE_INDEX:false,
      FILE_SUFFIX:'.html',
      LINK_SUFFIX: '.html',
      HAS_SOURCE:  true
    };
  </script>

  

  <script type="text/javascript" src="../_static/js/clipboard.min.js"></script>
  <script type="text/javascript" src="../_static/js/jquery.waypoints.min.js"></script>

  
  
  
  <script type="text/javascript">var CLIPPY_SVG_PATH = "../_static/clippy.svg";</script>
  <script type="text/javascript" src="../_static/js/custom.js"></script>
  

  
  
  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.StickyNav.enable();
    });

  </script>
  

  
  <script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        function copyToClipboard(text) {
            const textarea = document.createElement('textarea');
            textarea.value = text;
            document.body.appendChild(textarea);
            textarea.select();
            document.execCommand('copy');
            document.body.removeChild(textarea);
        }
        // Get the code block designator class entries
        const allHighlights = document.querySelectorAll('.highlight');
        // Disable copyable links for notebook cell numbering and for cell outputs
        const highlights = Array.from(allHighlights).filter(highlight => !highlight.closest('.highlight-none') && 
            !highlight.closest('.nboutput'));
    
        highlights.forEach(function(highlight) {
            const copyIcon = document.createElement('span');
            copyIcon.classList.add('copy-icon');
            copyIcon.innerHTML = '&#xf0ea;';

            copyIcon.addEventListener('click', function() {
                const code = highlight.querySelector('pre').textContent;
                copyToClipboard(code);

                // Flash effect on click
                this.style.color = '#0194E2';
                setTimeout(() => {
                    this.style.color = ''; 
                }, 100);

                // Display "Code copied to clipboard" near the clicked icon
                const message = document.createElement('span');
                message.textContent = "Copied!";
                message.classList.add('copy-message'); 

                // Append the message to the icon
                this.appendChild(message);

                setTimeout(() => {
                    this.removeChild(message);
                }, 500);
            });

            highlight.appendChild(copyIcon);
        });
    });
  </script>


<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function() {
        // Force download for notebook-download-btn
        const downloadButtons = document.querySelectorAll('.notebook-download-btn');
        downloadButtons.forEach(function(button) {
            button.addEventListener('click', function(event) {
                event.preventDefault(); // Prevent default behavior

                // Fetch the raw content of the notebook from GitHub
                fetch(button.href)
                    .then(response => response.blob())
                    .then(blob => {
                        const url = window.URL.createObjectURL(blob);
                        const link = document.createElement('a');
                        link.style.display = 'none';
                        link.href = url;
                        const filename = button.href.split('/').pop();
                        link.download = filename; 

                        document.body.appendChild(link);
                        link.click();

                        window.URL.revokeObjectURL(url);
                        document.body.removeChild(link);
                    })
                    .catch(err => console.error('Error fetching the notebook:', err));
            });
        });
    });
</script> 
</body>
</html>